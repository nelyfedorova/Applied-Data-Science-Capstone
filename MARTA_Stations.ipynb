{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](marta_Logo_of_the_Metropolitan_Atlanta_Rapid_Transit_Authority.png \"MARTA Logo\")\n",
    "\n",
    "<h1 align=center><font size = 5>Segmenting and Clustering MARTA Rail Stations in Atlanta</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "In this notebook, I explore and visualize data for venues around MARTA rail stations in Atlanta. First, I use Foursquare to get  popular venues near each station. Next, I use Foursquare categories hierarchy to assign each venue to a general category, thus significantly reducing data dimensionality. Then I explore primary and general venue categories for each station, and use TF-IDF normalization and LDA algorithm to generate  station labels. I also use K-means, DBSCAN and hierarchical clustering algorithms to group stations into category-related clusters. Finally, I visualize all my results in interactive Folium maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\" >\n",
    "\n",
    "<font size = 3>\n",
    "\n",
    "<b>[Introduction/Business Problem](#item000)</b>\n",
    "<p>\n",
    "<b>[1. MARTA Stations Data](#item100)</b> \n",
    "<p>\n",
    "    [1.1 Wikipedia Data](#item110)    \n",
    "                                        \n",
    "    [1.2 Foursquare Venue Data](#item120)      \n",
    "\n",
    "    [1.3 Foursquare Categories Hierarchy](#item130)  \n",
    "    \n",
    "    [1.4 Entries/Day vs Number of Venues](#item140)                                                                                \n",
    "<p>\n",
    "<b>[2. MARTA Stations Labels](#item200)</b>   \n",
    "<p>\n",
    "    [2.1 Venues with Most Traffic](#item210)   \n",
    "                                      \n",
    "    [2.2 Venues Specific for Each Station: TF-IDF](#item220)       \n",
    "    \n",
    "    [2.3 Venues Specific for Each Station: TF-IDF LDA](#item230)   \n",
    "    \n",
    "    [2.4 MARTA Word Cloud](#item240)                                       \n",
    "<p>\n",
    "<b>[3. MARTA Stations Clusters](#item300)</b>   \n",
    "<p>\n",
    "    [3.1 K-means](#item310)    \n",
    "\n",
    "    [3.2 DBSCAN](#item320)  \n",
    "    \n",
    "    [3.3 Hierarchical Clustering](#item330)     \n",
    "    \n",
    "    [3.4 Agglomerative Hierarchical Clustering](#item340)       \n",
    "<p>\n",
    "<b>[4. MARTA Stations Annotated Maps](#item400)</b>    \n",
    "<p>            \n",
    "<b>[Results](#item500)</b>   \n",
    "<p>            \n",
    "<b>[Discussion](#item600)</b>   \n",
    "<p>            \n",
    "<b>[Conclusion](#item700)</b>  \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction/Business Problem  <a class=\"anchor\" id=\"item000\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to explore venues associated with each of 38 MARTA rail stations in Atlanta, Georgia.  \n",
    "\n",
    "This might help us answer the following questions.\n",
    "\n",
    "Why people use MARTA?  \n",
    "What attracts them to each MARTA station and makes them go there?  \n",
    "Where do they go before\\after riding MARTA?   \n",
    "Where do they spend the most time (and money)?  \n",
    "Are MARTA stations similar or dissimilar in respect to the venues near them?  \n",
    "Can we label each MARTA station with venue categories that are specific for this station?  \n",
    "\n",
    "In our analysis, we will use Foursquare API to get the most popular venues near each MARTA station, TF-IDF normalization and LDA algorithm for labeling each station, k-means and hierarchical clustering for grouping similar stations, and Folium library to visualize our results.\n",
    "\n",
    "MARTA station names and their coordinates can be extracted from Wikipedia https://en.wikipedia.org/wiki/MARTA_rail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MARTA Stations Data  <a class=\"anchor\" id=\"item100\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get the data and start exploring it, let's download all the dependencies that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np # library to handle data in a vectorized manner\n",
    "\n",
    "import pandas as pd # library for data analsysis\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import json # library to handle JSON files\n",
    "\n",
    "#!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
    "\n",
    "import requests # library to handle requests\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "#!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "import folium # map rendering library\n",
    "\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Wikipedia Data <a class=\"anchor\" id=\"item110\"></a>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atlanta MARTA rail network has a total of 38 stations. In order to segement the stations and explore them, we will essentially need a dataset that contains the list of MARTA stations with the latitude and logitude coordinates of each station. \n",
    "\n",
    "Luckily, this data exists for free on Wikipedia: https://en.wikipedia.org/wiki/MARTA_rail\n",
    "It can be easily downloaded and converted to Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bFoursquare = False # False: read local csv file instead of foursquare.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stations = pd.read_excel('MARTA_stations.xlsx')\n",
    "print(stations.shape)\n",
    "stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a map of Atlanta with MARTA stations superimposed on top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use geopy library to get the latitude and longitude values of Downtown Atlanta. In order to define an instance of the geocoder, we need to define a user_agent. We will name our agent <em>atlanta_explorer</em>, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "address = 'Atlanta, GA'\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"atlanta_explorer\")\n",
    "location = geolocator.geocode(address)\n",
    "latitude = location.latitude\n",
    "longitude = location.longitude\n",
    "print('The geograpical coordinate of Downtown Atlanta are {}, {}.'.format(latitude, longitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize MARTA stations setting marker radius proportional to \"Entries/Day\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "map_downtown = folium.Map(location=[latitude, longitude], zoom_start=11)\n",
    "for lat, lng, label, num  in zip(stations['Latitude'], stations['Longitude'], stations['Station'], stations['Entries/Day']):\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius=num / 500,\n",
    "        popup=label,\n",
    "        color='red',\n",
    "        fill=True,\n",
    "        fill_color='#f4a8a7', #'#3186cc',\n",
    "        fill_opacity=0.7,\n",
    "        parse_html=False).add_to(map_downtown)  \n",
    "\n",
    "map_downtown.save('MARTA_map_Entries.html')\n",
    "map_downtown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Folium** is a great visualization library. Feel free to zoom into the above map, and click on each circle mark to reveal the name of the station."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Foursquare Venue Data <a class=\"anchor\" id=\"item120\"></a>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to start utilizing the Foursquare API to explore the stations and segment them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Foursquare Credentials and Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLIENT_ID = '???????????' # your Foursquare ID  \n",
    "CLIENT_SECRET = '??????????????' # your Foursquare Secret  \n",
    "VERSION = '20180605' # Foursquare API version  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# @hidden_cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's explore the first station in our dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the neighborhood's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ista = 33\n",
    "stations.loc[ista, 'Station']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the neighborhood's latitude and longitude values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "station_latitude = stations.loc[ista, 'Latitude']\n",
    "station_longitude = stations.loc[ista, 'Longitude'] \n",
    "station_name = stations.loc[ista, 'Station'] \n",
    "print('Latitude and longitude values of {} are {}, {}.'.format(station_name, \n",
    "                                                               station_latitude, \n",
    "                                                               station_longitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get the top 100 venues within a radius of 500 meters from this station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create the GET request URL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LIMIT = 500 # limit of number of venues returned by Foursquare API\n",
    "radius = 500 # define radius\n",
    "url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "    CLIENT_ID, \n",
    "    CLIENT_SECRET, \n",
    "    VERSION, \n",
    "    station_latitude, \n",
    "    station_longitude, \n",
    "    radius, \n",
    "    LIMIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send the GET request and convert json data into a *pandas* dataframe. All the information is in the *items* key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function that extracts the category of the venue\n",
    "def get_category_type(row):\n",
    "    try:\n",
    "        categories_list = row['categories']\n",
    "    except:\n",
    "        categories_list = row['venue.categories']\n",
    "        \n",
    "    if len(categories_list) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return categories_list[0]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if bFoursquare:\n",
    "    # query\n",
    "    results = requests.get(url).json()\n",
    "\n",
    "    # JSON\n",
    "    venues = results['response']['groups'][0]['items']\n",
    "\n",
    "    # flatten JSON\n",
    "    nearby_venues = json_normalize(venues) \n",
    "\n",
    "    # filter columns\n",
    "    filtered_columns = ['venue.id', 'venue.name', 'venue.categories', 'venue.location.lat', 'venue.location.lng']\n",
    "    nearby_venues =nearby_venues.loc[:, filtered_columns]\n",
    "\n",
    "    # filter the category for each row\n",
    "    nearby_venues['venue.categories'] = nearby_venues.apply(get_category_type, axis=1)\n",
    "\n",
    "    # clean columns\n",
    "    nearby_venues.columns = [col.split(\".\")[-1] for col in nearby_venues.columns]\n",
    "\n",
    "    print('{} venues were returned by Foursquare.'.format(nearby_venues.shape[0]))\n",
    "    nearby_venues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems Foursquare's API has a 100 result limit. We can get more results by using a nearby point. The results may overlap, so we will need to filter the duplicates out and keep changing the point by a tiny margin till the number of unique results stops growing or reaches specified LIMIT. We will leave it beyond the scope of this project for right now, and proceed with results limited to 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function to repeat the same process to all MARTA stations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNearbyVenues(names, latitudes, longitudes, radius=500):\n",
    "    \n",
    "    venues_list=[]\n",
    "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
    "        print(name)\n",
    "            \n",
    "        # create the API request URL\n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            radius, \n",
    "            LIMIT)\n",
    "            \n",
    "        # make the GET request\n",
    "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "        \n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            name, \n",
    "            lat, \n",
    "            lng, \n",
    "            v['venue']['id'], \n",
    "            v['venue']['name'], \n",
    "            v['venue']['location']['lat'], \n",
    "            v['venue']['location']['lng'],  \n",
    "            v['venue']['categories'][0]['name']) for v in results])\n",
    "\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_venues.columns = ['Station', \n",
    "                  'Station Latitude', \n",
    "                  'Station Longitude', \n",
    "                  'Venue ID', \n",
    "                  'Venue', \n",
    "                  'Venue Latitude', \n",
    "                  'Venue Longitude', \n",
    "                  'Venue Category']\n",
    "    \n",
    "    return(nearby_venues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we run the above function on each neighborhood and create a new dataframe called *venues_primary*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save venues_primary to a backup csv file so that we could re-use it later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if bFoursquare:\n",
    "    venues_primary = getNearbyVenues(names=stations['Station'],\n",
    "                                   latitudes=stations['Latitude'],\n",
    "                                   longitudes=stations['Longitude']\n",
    "                                  )\n",
    "    venues_primary.to_csv(\"MARTA_venues.csv\",index=False) \n",
    "    print(venues_primary.shape)\n",
    "    print(venues_primary.groupby('Station').count()) #venues were returned for each station\n",
    "    venues_primary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read venues_primary from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "venues_primary = pd.read_csv(\"MARTA_venues.csv\")\n",
    "stations = pd.read_excel('MARTA_stations.xlsx')\n",
    "print(venues_primary.shape)\n",
    "venues_primary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "venues_count = venues_primary[['Station','Venue']].groupby('Station').count().reset_index().set_index('Station')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize MARTA stations setting marker radius proportional to \"Venues\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "map_downtown = folium.Map(location=[latitude, longitude], zoom_start=11)\n",
    "for lat, lng, label, num, cnt  in zip(stations['Latitude'], stations['Longitude'], stations['Station'], stations['Entries/Day'], venues_count['Venue']):\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius=cnt / 5,\n",
    "        popup=label,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='#3186cc',\n",
    "        fill_opacity=0.7,\n",
    "        parse_html=False).add_to(map_downtown)  \n",
    "\n",
    "map_downtown.save('MARTA_map_Venues.html')\n",
    "map_downtown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all data ready for our analysis,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Foursquare Categories Hierarchy <a class=\"anchor\" id=\"item130\"></a>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out how many unique venues we have found. Some stations are located close to each other and can share some venues. This is OK for the purposes of our analysis, but let's check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = venues_primary.groupby(['Venue Latitude','Venue Longitude']).size().reset_index().rename(columns={0:'count'})\n",
    "print('There are {} uniques venues out of total {}.'.format(tmp.shape[0],venues_primary.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out how many unique categories can be curated from all the returned venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('There are {} uniques categories.'.format(len(venues_primary['Venue Category'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's encode our categories using One Hot method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(venues):\n",
    "    # one hot encoding\n",
    "    onehot = pd.get_dummies(venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "    # add neighborhood column back to dataframe\n",
    "    onehot['Station'] = venues['Station'] \n",
    "\n",
    "    # move Station column to the first column\n",
    "    fixed_columns = [onehot.columns[-1]] + list(onehot.columns[:-1])\n",
    "    onehot = onehot[fixed_columns]\n",
    "    \n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "primary_onehot = one_hot_encoding(venues_primary)\n",
    "print(primary_onehot.shape)\n",
    "primary_onehot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each row has exactly one non-zero value. Let's now group them by Station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "primary = primary_onehot.groupby('Station').sum().reset_index()\n",
    "primary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract General Categories "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data with 223 primary categories is very sparse. Let's get some more general categories so that we could merge, e.g. Airport Service and Airport Lounge into one general category Airport. We can use Foursquare categories hierarchy tree for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories_list = primary.columns.tolist()\n",
    "for x in range(len( categories_list)): \n",
    "    print(categories_list[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Foursquare categories hierarchy tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'https://api.foursquare.com/v2/venues/categories?&client_id={}&client_secret={}&v={}'.format(\n",
    "    CLIENT_ID, \n",
    "    CLIENT_SECRET, \n",
    "    VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories_tree = requests.get(url).json()\n",
    "categories_tree['response']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function to get tree path given a category name. It will return a list with category name in the very last element, and its root category in the first (0th) element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_category_path(json_tree, target_name, path):\n",
    "    for node in json_tree:                    \n",
    "        if type(node) == dict:\n",
    "            node_name = node.get(\"name\")\n",
    "            #print(\"path=\", path + [node_name])  \n",
    "            \n",
    "            for key,value in node.items():\n",
    "                if key == \"name\":\n",
    "                    #print(key,\" = \",value)\n",
    "                    if value == target_name:\n",
    "                        #print(\"FOUND \", key,\" = \",value)\n",
    "                        return path + [target_name]\n",
    "                elif key == \"categories\":\n",
    "                    check_child = get_category_path(value, target_name, path + [node_name])\n",
    "                    if check_child:\n",
    "                        return check_child\n",
    "\n",
    "print(get_category_path(categories_tree['response']['categories'], 'Aquarium', []))\n",
    "print(get_category_path(categories_tree['response']['categories'], 'Art Museum', []))\n",
    "print(get_category_path(categories_tree['response']['categories'], 'Asian Restaurant', []))\n",
    "print(get_category_path(categories_tree['response']['categories'], 'Japanese Restaurant', []))\n",
    "print(get_category_path(categories_tree['response']['categories'], 'Nightclub', []))\n",
    "print(get_category_path(categories_tree['response']['categories'], 'Whisky Bar', []))\n",
    "print(get_category_path(categories_tree['response']['categories'], 'Women\\'s Store', []))\n",
    "print(get_category_path(categories_tree['response']['categories'], 'Pharmacy', []))\n",
    "print(get_category_path(categories_tree['response']['categories'], 'Gas Station', []))\n",
    "print(get_category_path(categories_tree['response']['categories'], 'Park', []))\n",
    "print(get_category_path(categories_tree['response']['categories'], 'Yoga Studio', []))\n",
    "print(get_category_path(categories_tree['response']['categories'], 'Chiropractor', []))\n",
    "print(get_category_path(categories_tree['response']['categories'], 'Event Space', []))\n",
    "print(get_category_path(categories_tree['response']['categories'], 'College Basketball Court', []))\n",
    "print(get_category_path(categories_tree['response']['categories'], 'Residential Building (Apartment / Condo)', []))\n",
    "print(get_category_path(categories_tree['response']['categories'], 'Airport Lounge', []))\n",
    "print(get_category_path(categories_tree['response']['categories'], 'Light Rail Station', []))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to extract general category for each primary category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories_dict = {}\n",
    "general_list = categories_list.copy()\n",
    "for x in range(len(categories_list)): \n",
    "    a = get_category_path(categories_tree['response']['categories'], categories_list[x], [])\n",
    "    if a:\n",
    "        if len(a) >= 2:\n",
    "            a = a[0]; # immediate parent: a[-2];\n",
    "        else:\n",
    "            a = a[0]; # return itself\n",
    "        print(a)\n",
    "        categories_dict[categories_list[x]] = a\n",
    "    else:\n",
    "        a = \"\"\n",
    "    print(categories_list[x], \" --> \", a)\n",
    "    general_list[x] = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(np.unique(categories_list)))\n",
    "print(len(np.unique(general_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change category names in venues_primary[\"Venue Category\"] using categories_dict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original categories before renaming/merging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "venues_primary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categories after renaming into 9 general categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "venues_general = pd.read_csv(\"MARTA_venues.csv\")\n",
    "venues_general['Venue Category'] = venues_general['Venue Category'].replace(categories_dict)\n",
    "venues_general.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply One Hot encoding to this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "general_onehot = one_hot_encoding(venues_general)\n",
    "print(general_onehot.shape)\n",
    "general_onehot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "general = general_onehot.groupby('Station').sum().reset_index()\n",
    "general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the table with 223 primary categories, this table with only 9 columns is much easier to interprete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "general.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "general.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "general[general['College & University'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "general[general['Residence'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "general[general['Professional & Other Places'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, there are not so many Professional & Other Places venues in Foursquare results. Interestingly, tiny Kensington is also in this list, together with Buckhead and Peachtree Center. This is because Kensington's Utley Chiropractic (Utley Chiropractic & Wellness Center, to be precise) happens to fall into Professional & Other Places category.\n",
    "\n",
    "Anyhow, Food, Shop & Service and Travel & Transport categories dominate areas around all MARTA stations in Atlanta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = general.plot(kind='bar', stacked=True, figsize=(18.5, 10.5), cmap=\"Set3\") \n",
    "ax.set_ylabel('Number of Venues')\n",
    "plt.xticks(np.arange(38), list(general['Station']))\n",
    "plt.savefig('MARTA_stacked_general.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Entries/Day vs Number of Venues <a class=\"anchor\" id=\"item140\"></a>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot Entries/Day and number of venues for each station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def split_it(name):\n",
    "    return name.split('/')[0];\n",
    "split_it(\"Inman Park/Reynoldstown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add number of venues to stations\n",
    "merged = stations\n",
    "merged = merged.join(venues_primary[['Station','Venue']].groupby('Station').count().reset_index().set_index('Station'), on='Station')\n",
    "merged = merged.drop(['Latitude','Longitude'], axis=1)\n",
    "#merged['Station'] = merged['Station'].apply(split_it) \n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_scatter_entries_vs_venues(merged, ihighlights=[], name=\"\"):\n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "    ax = fig.add_subplot(1,1,1) \n",
    "    ax.set_xlabel('Number of Venues', fontsize = 10)\n",
    "    ax.set_ylabel('Entries/Day', fontsize = 10)\n",
    "    ax.set_title('MARTA Stations', fontsize = 15)\n",
    "    ax.grid()\n",
    "    \n",
    "    targets = ['no parking', 'free daily', 'free daily & long-term']\n",
    "    itargets = [0, 1, 2]\n",
    "    markers = ['o', '^', 's']\n",
    "    marker_colors = ['b', 'm', 'r']\n",
    "\n",
    "    if len(ihighlights) == merged.shape[0]:\n",
    "        # Interprete ihighlights as cluster labels \n",
    "        kclusters = np.unique(ihighlights).shape[0]\n",
    "        xs = np.arange(kclusters)\n",
    "        ys = [i + xs + (i*xs)**2 for i in range(kclusters)]\n",
    "        colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "        rainbow = [matplotlib.colors.rgb2hex(i) for i in colors_array]    \n",
    "        \n",
    "    for target, color, mark in zip(itargets,marker_colors,markers):\n",
    "        indicesToKeep = (merged['Parking'] == target)\n",
    "        ax.scatter(merged.loc[indicesToKeep, 'Venue']\n",
    "                   , merged.loc[indicesToKeep, 'Entries/Day']\n",
    "                   , c = color\n",
    "                   , marker = mark\n",
    "                   , s = 50)        \n",
    "    ax.legend(targets)\n",
    "    \n",
    "    ix = merged.columns.get_loc(\"Venue\")\n",
    "    iy = merged.columns.get_loc(\"Entries/Day\")\n",
    "    for i in range(merged.shape[0]):\n",
    "        if len(ihighlights) == merged.shape[0]:\n",
    "            # Interprete ihighlights as cluster labels \n",
    "            #ax.scatter(merged.iloc[i,ix], merged.loc[i,iy], c = rainbow[ihighlights[i]-1], marker = 'o', s = 50)        \n",
    "            ax.annotate(split_it(merged.iloc[i,0]), xy=(merged.iloc[i,ix]+1, merged.iloc[i,iy]), color=rainbow[ihighlights[i]-1])\n",
    "        else:\n",
    "            if i in ihighlights:\n",
    "                ax.annotate(split_it(merged.iloc[i,0]), xy=(merged.iloc[i,ix]+1, merged.iloc[i,iy]), color=\"black\")\n",
    "            else:\n",
    "                ax.annotate(split_it(merged.iloc[i,0]), xy=(merged.iloc[i,ix]+1, merged.iloc[i,iy]), color=\"grey\")\n",
    "    \n",
    "    plt.xlim(0,120)\n",
    "    plt.ylim(0,20000)\n",
    "\n",
    "    # Separator of no-parking from free parking station\n",
    "    plt.plot([26,26], [0,20000], color='red')\n",
    "    \n",
    "    # Save and show\n",
    "    plt.savefig('MARTA_Entries_vs_Venues_'+name+'.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot_scatter_entries_vs_venues(merged, [], '')\n",
    "plot_scatter_entries_vs_venues(merged, [3,6,10,11,17,19,22,23,26,29,33], 'Professional')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows 3 groups of stations: no parking (blue), with free daily parking (magenta) and with free daily & long-term parking (red). Stations with Professionl Services are shown in bold. \n",
    "\n",
    "Some observations:\n",
    "\n",
    "- Number of venues is not correlated with Entries/Day, but is related to free parking. \n",
    "\n",
    "- A simple rule \"Number of Venues <= 26\" surprisingly well separates stations with parking from those with no parking. There are only 3 exceptions with parking (Dunwoody, Lindbergh Center, Inman Park) and 2 exceptions with no parking (Bankhead and Vine City). Lenox is near the boundary. It seems that MARTA stations with parking are mostly used by park-and-ride commuters. \n",
    "\n",
    "- Indian Creek, Kensington, College Park and North Springs have many Entries/Day, but a very small number of venues. Indian Creek has only 3 venues: Bus Station,Light Rail Station,Park. Kensington has 4 venues: Gas Station, Discount Store, Pharmacy, Chiropractor\n",
    "\n",
    "- Five Points, which is a transit point between all four MARTA Rail lines, has the highest number of Entries per Day of all MARTA stations, but only 33 venues. \n",
    "\n",
    "- Buckhead, Midtown and Peachtree Center have average to high Entries/Day, and are surrounded with very many venues. There are also smaller stations like this, e.g. King Memorial, Dome, Dunwoody, Decatur, North Avenue, Arts Center, Lindbergh Center. All these stations seem to be located in dense areas with many different businesses.\n",
    "\n",
    "- Bankhead and West Lake are very small both in number of venues and in Entries/Day. Bankhead has 3 venues: Gas Station, Park, Ice Cream Shop. West Lake has 6 venues: 2 Gas Stations, Light Rail Station, Bus Station, Wings Joint, CafÃ©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d0 = merged.loc[(merged['Parking'] == 0), 'Venue'].as_matrix()\n",
    "d1 = merged.loc[(merged['Parking'] != 0), 'Venue'].as_matrix()\n",
    "data_to_plot = [d1, d0]\n",
    "\n",
    "mpl_fig = plt.figure()\n",
    "ax = mpl_fig.add_subplot(111)\n",
    "ax.boxplot(data_to_plot)\n",
    "plt.title('Number of venues around MARTA stations')\n",
    "ax.set_xticklabels(['free parking','no parking'])\n",
    "plt.savefig('MARTA_Venues_boxplot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 MARTA Stations Labels <a class=\"anchor\" id=\"item200\"></a>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Venues with Most Traffic <a class=\"anchor\" id=\"item210\"></a>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's group venues by station and take the mean of the frequency of occurrence of each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "general = general_onehot.groupby('Station').mean().reset_index()\n",
    "print(general.shape)\n",
    "general.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "primary = primary_onehot.groupby('Station').mean().reset_index()\n",
    "print(primary.shape)\n",
    "primary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print each station along with the top 5 most common venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_top_categories(grouped=primary, num_top_venues=5):\n",
    "    for hood in grouped['Station']:\n",
    "        print(\"----\"+hood+\"----\")\n",
    "        temp = grouped[grouped['Station'] == hood].T.reset_index()\n",
    "        temp.columns = ['venue','freq']\n",
    "        temp = temp.iloc[1:]\n",
    "        temp['freq'] = temp['freq'].astype(float)\n",
    "        temp = temp[temp.freq > 0] # print inly non-zero frequencies\n",
    "        temp = temp.round({'freq': 2})\n",
    "        num_top = np.min([num_top_venues,len(temp)])\n",
    "        print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_top_categories(grouped=general, num_top_venues=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_top_categories(grouped=primary, num_top_venues=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write functions to sort the venues in descending order and display the top 10 venues for each station in a more compact tabular form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_most_common_venues(row, num_top_venues):\n",
    "    row_categories = row.iloc[1:]\n",
    "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
    "    tmp = row_categories_sorted.index.values[0:num_top_venues]\n",
    "    for i in range(num_top_venues):\n",
    "        if row_categories_sorted[i] <= 0:\n",
    "            tmp[i] = '-' \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top_venues(grouped, num_top_venues, subset, bshorten_names=False):\n",
    "\n",
    "    num_top = np.min([num_top_venues, grouped.shape[1]-1])\n",
    "    \n",
    "    indicators = ['st', 'nd', 'rd']\n",
    "\n",
    "    # create columns according to number of top venues\n",
    "    columns = ['Station']\n",
    "    for ind in np.arange(num_top):\n",
    "        try:\n",
    "            columns.append('{}{} Most Common Category'.format(ind+1, indicators[ind]))\n",
    "        except:\n",
    "            columns.append('{}th Most Common Category'.format(ind+1))\n",
    "\n",
    "    # create a new dataframe\n",
    "    venues_sorted = pd.DataFrame(columns=columns)\n",
    "    venues_sorted['Station'] = grouped['Station']\n",
    "    \n",
    "    # shorten station names if required\n",
    "    if bshorten_names:\n",
    "        venues_sorted['Station'] = venues_sorted['Station'].apply(split_it)\n",
    "\n",
    "    # get sorted venues for each row\n",
    "    if not subset:\n",
    "        for ind in np.arange(grouped.shape[0]):\n",
    "            venues_sorted.iloc[ind, 1:] = return_most_common_venues(grouped.iloc[ind, :], num_top)\n",
    "    else:\n",
    "        for ind in np.arange(grouped.shape[0]):\n",
    "            if ind in subset:\n",
    "                venues_sorted.iloc[ind, 1:] = return_most_common_venues(grouped.iloc[ind, :], num_top)\n",
    "        venues_sorted = venues_sorted.loc[subset]\n",
    "        \n",
    "    return venues_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "general_top = top_venues(general, 10, [])\n",
    "general_top.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "primary_top = top_venues(primary, 10, [])\n",
    "primary_top.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table shows most popular venues, with largest foot traffic.  We can see that these are mostly fast food eateries and coffee shops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a closer look at some stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_venues(general, 10, [0,1,6,17,19,27])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_venues(primary, 10, [0,1,6,17,19,27])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Venues Specific for Each Stations: TF-IDF <a class=\"anchor\" id=\"item220\"></a>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see what makes each MARTA station special and distinct from other stations.\n",
    "\n",
    "We can do it by using Term Frequency-Inverse Document Frequency (TF-IDF) and similar techniques that are widely used in text processing for normalizing word counts in a corpus of N documents. In our case \"word\" is a category, and \"document\" is a station. Each station (\"document\") can be viewed as a bag of categories (\"words\") of all venues near that station. \n",
    "\n",
    "- Term Frequency (TF) = (Number of times a word w appears in a document d)/(Total number of words in the document d)\n",
    "\n",
    "- Inverse Document Frequency (IDF) = -log(n/N), where, N is the total number of documents in the corpus and n is the number of documents a word w has appeared in. The IDF of a rare word is high, whereas the IDF of a frequent word is likely to be low.\n",
    "\n",
    "- We calculate TF-IDF value of a word w in document d as = TF * IDF\n",
    "\n",
    "Due to the IDF term, TF-IDF method is expected to heavily penalize common words/categories like \"Coffee Shop\" but assign greater weight to categories like \"Airport Lounge\" and \"Aquarium\" that are more distinct and distinguish each station from the other 38 stations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's normalize our data using TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_tfidf(primary_tfidf):\n",
    "    # Drop names\n",
    "    tmp = primary_tfidf.loc[:, primary_tfidf.columns != 'Station']\n",
    "    \n",
    "    # Devide by the row sum  \n",
    "    TF = tmp.div(tmp.sum(axis=1), axis=0)\n",
    "    \n",
    "    # Count non zeroes in each column\n",
    "    IDF = -np.log(tmp.astype(bool).sum(axis=0, skipna = True) / tmp.shape[0])\n",
    "    \n",
    "    # TF * IDF\n",
    "    tmp = TF * IDF\n",
    "    \n",
    "    # Devide by the row sum again so that all values looked like probabilities\n",
    "    tmp = tmp.div(tmp.sum(axis=1), axis=0)\n",
    "   \n",
    "    # Add names back\n",
    "    primary_tfidf = pd.concat([primary_tfidf[[\"Station\"]], tmp], axis=1)\n",
    "    \n",
    "    return primary_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "general_tfidf = normalize_tfidf(general_onehot.groupby('Station').sum().reset_index()) \n",
    "print(general_tfidf.shape)\n",
    "general_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "primary_tfidf = normalize_tfidf(primary_onehot.groupby('Station').sum().reset_index()) \n",
    "print(primary_tfidf.shape)\n",
    "primary_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_top_categories(grouped=general_tfidf, num_top_venues=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_top_categories(grouped=primary_tfidf, num_top_venues=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "general_top_tfidf = top_venues(general_tfidf, 10, [])\n",
    "general_top_tfidf.to_csv('MARTA_general_top.csv')\n",
    "general_top_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "primary_top_tfidf = top_venues(primary_tfidf, 10, [])\n",
    "general_top_tfidf.to_csv('MARTA_primary_top.csv')\n",
    "primary_top_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_venues(general_tfidf, 10, [0,1,6,17,19,27])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_venues(primary_tfidf, 10, [0,1,6,17,19,27])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the difference in the top 10 categories for each station, especially for Airport, Arts Center and Georgia State. Fast food venues are now rated lower for all stations and categories like Airport Service and Performing Arts Venue went to the top. Even for Five Points fast food places gave way to less common Hookah Bar and Cuban Restaurant. Compared to mean frequencies, TF-IDF sorting is much more suitable for labeling\\annotating stations. \n",
    "\n",
    "Notice also that TF-IDF labels based on primary categories are very specific, e.g. Cuban Restaurant instead of general Food. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to view this data is to merge all categories columns in one string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top_venues_text(top):\n",
    "    top_text = top.iloc[:,1:].apply(lambda row: ', '.join(row.values.astype(str)), axis=1).reset_index()\n",
    "    top_text.insert(0, 'Station', list(top['Station']))\n",
    "    top_text.reset_index().set_index('Station')\n",
    "    top_text = top_text.drop(['index'], axis=1)\n",
    "    top_text.columns = ['Station','Top10']\n",
    "    top_text['Top10'] = top_text['Top10'].str.replace(', -', '')\n",
    "    return top_text   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "primary_top10 = top_venues_text(primary_top)\n",
    "primary_top10.to_csv(\"MARTA_primary_top10.csv\")\n",
    "primary_top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "primary_top10_tfidf = top_venues_text(primary_top_tfidf)\n",
    "primary_top10_tfidf.to_csv(\"MARTA_primary_top10_tfidf.csv\")\n",
    "primary_top10_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Venues Specific for Each Stations: TF-IDF LDA<a class=\"anchor\" id=\"item230\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use LDA topic modelling technique to label the stations based on their primary categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's merge all primary categories names and get text \"documents\" that descrcibe each station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "primary_list = venues_primary.groupby(['Station'])['Venue Category'].apply(list).reset_index()\n",
    "primary_list = primary_list.drop('Station', 1)\n",
    "primary_list = primary_list.T.squeeze()\n",
    "primary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "primary_text = venues_primary.groupby(['Station'])['Venue Category'].apply(' '.join).reset_index()\n",
    "primary_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA usually require some text preprocessing (tokenizing, lemmatizing, stemming, etc.). We will use nltk and gensim libraries for this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_sample = primary_text.iloc[0,1]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 2 options: either consider each document consisting of primary categories (and use primary_list) or break it down to the word level (and use preprocessed primary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    primary_docs = primary_text['Venue Category'].map(preprocess)\n",
    "else:\n",
    "    primary_docs = primary_list\n",
    "primary_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(primary_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "#dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in primary_docs]\n",
    "\n",
    "bow_doc = bow_corpus[0]\n",
    "for i in range(len(bow_corpus[0])):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc[i][0], dictionary[bow_doc[i][0]], bow_doc[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lda_topics_words(lda_model):\n",
    "    lda_topics = pd.DataFrame(columns=['Topic','Words']) \n",
    "    lda_topics.set_index('Topic')\n",
    "    for idx, topic in lda_model.print_topics(-1):\n",
    "        lda_topics = lda_topics.append({'Topic':'Topic'+str(idx), 'Words':topic}, ignore_index=True)\n",
    "    return lda_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running LDA using word counts (bag of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)\n",
    "lda_topics = lda_topics_words(lda_model)\n",
    "lda_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "lda_topics_tfidf = lda_topics_words(lda_model_tfidf)\n",
    "lda_topics_tfidf.to_csv(\"MARTA_lda_topics.csv\")\n",
    "lda_topics_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use LDA TF-IDF model to classify some station (e.g. 0th, the Airport):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "istation = 1\n",
    "print(stations.iloc[istation,0])\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[istation]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t Topic{}: {}\".format(score, index, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print topic(s) for each station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top_venues_lda(lda_model, stations, num_top_topics):\n",
    "    columns = ['Station']\n",
    "    for ind in np.arange(num_top_topics):\n",
    "        columns.append('Topic{}'.format(ind))\n",
    "    topics_lda = pd.DataFrame(columns=columns)\n",
    "    topics_lda.set_index('Station')\n",
    "\n",
    "    for istation, station in enumerate(stations['Station']):\n",
    "        tmp = lda_model[bow_corpus[istation]]\n",
    "        di = {'Station': station} \n",
    "        for index, score in sorted(lda_model_tfidf[bow_corpus[istation]], key=lambda tup: -1*tup[1]):\n",
    "            key = 'Topic{}'.format(index)\n",
    "            di1 = {key: score}\n",
    "            di = {**di, **di1}\n",
    "        topics_lda = topics_lda.append(di, ignore_index=True)\n",
    "    topics_lda.fillna(0, inplace=True)\n",
    "    return topics_lda\n",
    "\n",
    "primary_lda_tfidf = top_venues_lda(lda_model_tfidf, stations, 10)\n",
    "primary_lda_tfidf.to_csv(\"MARTA_primary_lda_tfidf.csv\")\n",
    "primary_lda_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 MARTA Word Cloud <a class=\"anchor\" id=\"item240\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "def plot_word_cloud(text):\n",
    "    if False:\n",
    "        # Create and generate a word cloud image:\n",
    "        wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\n",
    "        plt.figure()\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "    else:\n",
    "        # Use colors from MARTA logo\n",
    "        stopwords = []\n",
    "        mask = np.array(Image.open(\"marta-squarelogo.png\"))\n",
    "        wordcloud_marta = WordCloud(stopwords=stopwords, background_color=\"white\", max_words=1000, mask=mask).generate(doc_sample)\n",
    "        # create coloring from image\n",
    "        image_colors = ImageColorGenerator(mask)\n",
    "        plt.figure(figsize=[7,7])\n",
    "        plt.imshow(wordcloud_marta.recolor(color_func=image_colors), interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "    # store to file and show\n",
    "    plt.savefig(\"MARTA_wordcloud.png\", format=\"png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start with one station:\n",
    "doc_sample = primary_text.iloc[0,1]\n",
    "plot_word_cloud(doc_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# All stations\n",
    "tmp = primary_text['Venue Category']\n",
    "tmp = tmp.T.squeeze()\n",
    "doc_sample = tmp.str.cat(sep=', ')\n",
    "plot_word_cloud(doc_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stations with no parking\n",
    "tmp = primary_text['Venue Category']\n",
    "tmp = tmp[stations['Parking'] == 0]\n",
    "tmp = tmp.T.squeeze()\n",
    "doc_sample = tmp.str.cat(sep=', ')\n",
    "plot_word_cloud(doc_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stations with parking\n",
    "tmp = primary_text['Venue Category']\n",
    "tmp = tmp[stations['Parking'] != 0]\n",
    "tmp = tmp.T.squeeze()\n",
    "doc_sample = tmp.str.cat(sep=', ')\n",
    "plot_word_cloud(doc_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 MARTA Stations Clusters <a class=\"anchor\" id=\"item300\"></a>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MARTA rail system is not very big. We really don't need any clustering for only 38 stations, 2/3 of which are park-and-ride stations with hardly any venues other than fast food restaurants around them. \n",
    "\n",
    "However, for educational purposes let's try to use TF-IDF frequencies to group MARTA stations into some category-related clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop station names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "general_data = general_tfidf.drop('Station', 1)\n",
    "general_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "primary_data = primary_tfidf.drop('Station', 1)\n",
    "primary_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Euclidean distances between stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def plot_euclidean(data):\n",
    "    num = data.shape[0]\n",
    "    w = np.zeros((num,num))\n",
    "    for i in range(num):\n",
    "        for j in range(num):\n",
    "            w[i,j] = distance.sqeuclidean(list(data.iloc[i]), list(data.iloc[j]))\n",
    "\n",
    "    im = plt.imshow(w, cmap='hot', interpolation='nearest')\n",
    "    plt.colorbar(im)\n",
    "    plt.title('Euclidean')\n",
    "    plt.savefig('MARTA_Euclidean'+str(data.shape[1])+'.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_euclidean(general_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_euclidean(primary_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, all distances are very similar and close to 0, except for Bankhead, Indian Creek, Kensington, West Lake stations (indeces: [4, 21, 23, 37]). \n",
    "\n",
    "With a distance matrix like this K-means and other clustering algorithms are very likely to separate these 4 stations and consider the other 34 stations as one big cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bShowMetrices = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider other distances. For example, let's compute pairwise Wasserstein distances between stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_wasserstein(data):\n",
    "    num = data.shape[0]\n",
    "    w = np.zeros((num,num))\n",
    "    for i in range(num):\n",
    "        for j in range(num):\n",
    "            w[i,j] = wasserstein_distance(list(data.iloc[i]), list(data.iloc[j]))\n",
    "\n",
    "    im = plt.imshow(w, cmap='hot', interpolation='nearest')\n",
    "    plt.colorbar(im);\n",
    "    plt.title('Wasserstein');\n",
    "    plt.savefig('MARTA_Wasserstein'+str(data.shape[1])+'.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if bShowMetrices:\n",
    "    plot_wasserstein(general_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if bShowMetrices:\n",
    "    plot_wasserstein(primary_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the Hamming distance between stations, which is simply the proportion of disagreeing components in 2 vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_hamming(data):\n",
    "    num = data.shape[0]\n",
    "    w = np.zeros((num,num))\n",
    "    for i in range(num):\n",
    "        for j in range(num):\n",
    "            w[i,j] = distance.hamming(list(data.iloc[i]), list(data.iloc[j]))\n",
    "\n",
    "    im = plt.imshow(w, cmap='hot', interpolation='nearest')\n",
    "    plt.colorbar(im);\n",
    "    plt.title('Hamming');\n",
    "    plt.savefig('MARTA_Hamming'+str(data.shape[1])+'.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if bShowMetrices:\n",
    "    plot_hamming(general_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if bShowMetrices:\n",
    "    plot_hamming(primary_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 K-Means <a class=\"anchor\" id=\"item310\"></a>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will analyze the K-Means with elbow method. Note that sklearn implementation of K-Means uses only Euclidean distances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import k-means from clustering stage\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def kmeans_elbow(data):\n",
    "    distortions = []\n",
    "    K = range(1,20)\n",
    "    for k in K:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0).fit(data)\n",
    "        distortions.append(kmeans.inertia_)\n",
    "\n",
    "    # Plot the elbow\n",
    "    plt.plot(K, distortions, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Distortion')\n",
    "    plt.title('The Elbow Method showing the optimal k')\n",
    "    plt.savefig('MARTA_KMeans_Elbow'+str(data.shape[1])+'.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans_elbow(general_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans_elbow(primary_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no clear elbow in both plots. It suggests that there might be no meaningful clusters in our data. No elbow might also mean that the algorithm used cannot separate clusters (e.g. when K-means is used for concentric circles, vs DBSCAN). \n",
    "\n",
    "Nevertheless, let's run K-Means to cluster the stations and explore these clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kmeans(data, k):\n",
    "    # run k-means clustering\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(data)\n",
    "\n",
    "    # check cluster labels generated for each row in the dataframe\n",
    "    cluster_labels = kmeans.labels_\n",
    "    cluster_centers = kmeans.cluster_centers_\n",
    "    print(cluster_labels)\n",
    "    return cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_clusters(grouped, cluster_labels, num_top_venues):\n",
    "    kclusters = np.unique(cluster_labels).shape[0]\n",
    "    for cluster in range(kclusters):\n",
    "        print('-----Cluster ', cluster, '-----') \n",
    "        print(top_venues(grouped, num_top_venues, np.where(cluster_labels == cluster)[0].tolist(), True)) # bshorten_names=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "general_kmeans = kmeans(general_data, 9)\n",
    "print_clusters(general_tfidf, general_kmeans, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, actually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "primary_kmeans = kmeans(primary_data, 9)\n",
    "print_clusters(primary_tfidf, primary_kmeans, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use Entries/Day, number of venues, Parking and other data for clustering, together with TF-IDF frequencies. We will just need to normalize data matrix before feeding it to Kmeans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "clustering_data = StandardScaler().fit_transform(merged[[\"Parking\",\"Venue\",\"Entries/Day\"]])\n",
    "kmeans_elbow(clustering_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parking_clusters = kmeans(clustering_data, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with some other clustering algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bPrintClusters = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 DBSCAN <a class=\"anchor\" id=\"item320\"></a>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import sklearn.utils\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sklearn.utils.check_random_state(1000)\n",
    "\n",
    "sorted(sklearn.neighbors.VALID_METRICS['brute'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dbsc(data):\n",
    "    # Compute DBSCAN hamming\n",
    "    dbscan_dataSet = StandardScaler().fit_transform(data)\n",
    "    db = DBSCAN(eps=0.15, min_samples=1, metric='hamming').fit(dbscan_dataSet)\n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "    cluster_labels = db.labels_\n",
    "    print(cluster_labels)\n",
    "    return cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "general_dbsc = dbsc(general_data)\n",
    "if bPrintClusters:\n",
    "    print_clusters(general_tfidf, general_dbsc, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "primary_dbsc = dbsc(primary_data)\n",
    "if bPrintClusters:\n",
    "    print_clusters(primary_tfidf, primary_dbsc, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Hierarchical clustering <a class=\"anchor\" id=\"item330\"></a>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately we can not pass custom distance metrics (e.g. Wasserstein distance) to Sklearn.KMeans. But we can do it with fclusterdata. This function performs hierarchical clustering using the single linkage algorithm, and forms flat clusters using the inconsistency method with t=1.0 as the cut-off threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import fclusterdata\n",
    "\n",
    "def fclust(data):\n",
    "    #fclust = fclusterdata(primary_data, 1.0, metric='euclidean')\n",
    "    fclust = fclusterdata(data, 1.0, metric=wasserstein_distance)\n",
    "    cluster_labels = fclust - 1\n",
    "    print(cluster_labels)\n",
    "    #print(np.allclose(fclust1, fclust2))\n",
    "    return cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "general_fclust = fclust(general_data)\n",
    "if bPrintClusters:\n",
    "    print_clusters(general_tfidf, general_fclust, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "primary_fclust = fclust(primary_data)\n",
    "if bPrintClusters:\n",
    "    print_clusters(primary_tfidf, primary_fclust, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Agglomerative Hierarchical Clustering <a class=\"anchor\" id=\"item340\"></a>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "def aggl(data):\n",
    "    aggl = AgglomerativeClustering(n_clusters=10, affinity='hamming', linkage='complete')\n",
    "    cluster_labels = aggl.fit_predict(data) \n",
    "    print(cluster_labels) \n",
    "    return cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "general_aggl = dbsc(general_data)\n",
    "if bPrintClusters:\n",
    "    print_clusters(general_tfidf, general_aggl, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "primary_aggl = dbsc(primary_data)\n",
    "if bPrintClusters:\n",
    "    print_clusters(primary_tfidf, primary_aggl, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 4. MARTA Stations Annotated Maps <a class=\"anchor\" id=\"item400\"></a>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our results using Folium maps.\n",
    "\n",
    "We can create many different maps, for example:\n",
    "\n",
    "- general categories clusters and general categories labels\n",
    "- primary categories clusters and primary categories labels\n",
    "- general categories clusters and primary categories labels\n",
    "- parking data (parking/no parking) and general categories labels, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "address = 'Atlanta, GA'\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"atlanta_explorer\")\n",
    "location = geolocator.geocode(address)\n",
    "latitude = location.latitude\n",
    "longitude = location.longitude\n",
    "print('The geograpical coordinate of Downtown Atlanta are {}, {}.'.format(latitude, longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "def show_map_clusters(top, stations, venues, cluster_labels, bvenues, map_name):\n",
    "    \n",
    "    # Merge all station data into one dataframe\n",
    "    merged = stations\n",
    "    # add latitude/longitude for each neighborhood\n",
    "    merged = merged.join(top.set_index('Station'), on='Station')\n",
    "    # add venues\n",
    "    merged = merged.join(venues, on='Station')\n",
    "    # add clustering labels\n",
    "    #if 'Cluster Labels' in merged.columns:\n",
    "    merged['Cluster Labels'] = cluster_labels\n",
    "    #else:\n",
    "    #    merged.insert(2, 'Cluster Labels', cluster_labels)\n",
    "   \n",
    "    map_clusters = folium.Map(location=[latitude, longitude], zoom_start=11)\n",
    "\n",
    "    # set color scheme for the clusters\n",
    "    # MARTA Orange: #ea7d09 rgb(234, 125, 9)\n",
    "    # MARTA Blue: #3e78da rgb(62, 120, 218)   \n",
    "    kclusters = np.unique(cluster_labels).shape[0]\n",
    "    counts = np.bincount(cluster_labels)\n",
    "    common_cluster = np.argmax(counts)\n",
    "    bcommon_cluster = np.max(counts) >= len(cluster_labels)/2\n",
    "    \n",
    "    xs = np.arange(kclusters)\n",
    "    ys = [i + xs + (i*xs)**2 for i in range(kclusters)]\n",
    "    colors_array = cm.rainbow(np.linspace(0, 1, len(ys)))\n",
    "    rainbow = [matplotlib.colors.rgb2hex(i) for i in colors_array]\n",
    "    \n",
    "    # add markers to the map\n",
    "    markers_colors = []\n",
    "    for lat, lon, poi, top1, top2, cluster, num, cnt in zip(merged['Latitude'], merged['Longitude'], merged['Station'], \n",
    "                merged['1st Most Common Category'], merged['2nd Most Common Category'], merged['Cluster Labels'], \n",
    "                merged['Entries/Day'], merged['Venue']):    \n",
    "        \n",
    "        if bvenues:\n",
    "            rad = cnt / 5\n",
    "        else:\n",
    "            rad = num / 500\n",
    "        label = folium.Popup(split_it(poi) + ': ' + top1 + ', ' + top2, parse_html=True)\n",
    "        tooltip = label\n",
    "        if bcommon_cluster:\n",
    "            if cluster != common_cluster: \n",
    "                color = 'blue' #  '#3e78da' \n",
    "                fill_color = '#3186cc' \n",
    "            else:\n",
    "                color = \"red\" \n",
    "                fill_color = '#f4a8a7'\n",
    "        else:\n",
    "            color = rainbow[cluster-1]\n",
    "            fill_color = rainbow[cluster-1]\n",
    "\n",
    "        folium.CircleMarker(\n",
    "            [lat, lon],\n",
    "            radius=rad,\n",
    "            popup=label,\n",
    "            tooltip=tooltip,\n",
    "            color=color,\n",
    "            fill=True,\n",
    "            fill_color=fill_color,\n",
    "            fill_opacity=0.7).add_to(map_clusters)\n",
    "    \n",
    "    map_clusters.save('MARTA_map_'+map_name+str(int(bvenues))+'.html')\n",
    "    return map_clusters, merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Number of stations with <  26 venues: \", venues_count[venues_count['Venue'] < 26].count())\n",
    "print(\"Number of stations with >= 26 venues: \", venues_count[venues_count['Venue'] >= 26].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "general_map, general_merged_tfidf = show_map_clusters(general_top_tfidf, stations, venues_count, general_kmeans, True, \"general_kmeans\")\n",
    "general_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "primary_map, primary_merged_tfidf = show_map_clusters(primary_top_tfidf, stations, venues_count, primary_kmeans, True, \"primary_kmeans\")\n",
    "primary_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hybrid_map, hybrid_merged_tfidf = show_map_clusters(primary_top_tfidf, stations, venues_count, general_kmeans, True, \"hybrid_kmeans\")\n",
    "hybrid_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parking_map, parking_merged_tfidf = show_map_clusters(general_top_tfidf, stations, venues_count, \n",
    "                                                      list(stations['Parking'].map({0: 0, 1: 1, 2: 1})), True, \"general_parking\")\n",
    "parking_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parking_map, parking_merged_tfidf = show_map_clusters(primary_top_tfidf, stations, venues_count, \n",
    "                                                      list(stations['Parking'].map({0: 0, 1: 1, 2: 1})), True, \"primary_parking\")\n",
    "parking_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parking_map, parking_merged_tfidf = show_map_clusters(general_top_tfidf, stations, venues_count, \n",
    "                                                      parking_clusters, True, \"clustering_parking\")\n",
    "parking_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can click on each station and see its label\\annotation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results <a class=\"anchor\" id=\"item500\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-\tI used Foursquare to retrieve 223 primary and 9 general categories for 1156 popular venues near 38 MARTA stations\n",
    "-\tI analyzed Entries/Day and number of venues for each MARTA station\n",
    "-\tI discovered that number of nearby venues is strongly correlated with free parking\n",
    "-\tI discovered that MARTA stations can be roughly divided into two major groups: park-and-ride stations, with < 26 venues, and urban stations with no parking and â¥ 26 venues. Three important exceptions are Dunwoody, Lindbergh Center and Inman Park/Reynoldstown located in dense areas. These stations have free parking and many venues nearby\n",
    "-\tI discovered that fast food restaurants, coffee shops and retail shops prevail in areas around all MARTA stations\n",
    "-\tI used TF-IDF and LDA algorithms to extracted venue categories that are most specific\\discriminative for each station and used these categories to label\\annotate each station\n",
    "-\tI used K-means and other clustering algorithms to group MARTA stations into ~10 clusters based on their venues categories\n",
    "-\tI visualized MARTA data on interactive Folium maps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion <a class=\"anchor\" id=\"item600\"></a>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for Buckhead, Midtown and Peachtree Center are incomplete, since these stations reached Foursquare APIâs venue result limit of 100. We can get more Foursquare results by using nearby points. The results may overlap, so we will need to filter the duplicates out and keep changing the point by a tiny margin till the number of unique venues for each station stops growing or reaches some limit.\n",
    "\n",
    "MARTA venue analysis can be enriched also by using secondary categories, foot traffic and other detailed information for each venue. This require Foursquare primary calls, so I left it beyond the scope of this small project.\n",
    "\n",
    "Category merging using Foursquare category tree also has much room for improvement. For example, parent category for \"Pharmacy\" is \"Shop & Service\". This makes Medical Center look like any other station.\n",
    "\n",
    "MARTA station labeling is basically a text document labeling/annotation and topic modeling problem. There are many algorithms for this, with many tunable parameters. It would be interesting, for example, to vary number of topics and build LDA models with different detalization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                               \n",
    "# Conclusion <a class=\"anchor\" id=\"item700\"></a>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I analyzed popular venues near 38 MARTA rail stations in Atlanta, Georgia.  \n",
    "\n",
    "My analysis revealed very low business activity (less than 26 venues) around 18 out of 22 MARTA stations with free parking. Three important exceptions are Dunwoody (I-285/SR 400 interchange), Lindbergh Center (I-85/SR 400 interchange) and Inman Park/Reynoldstown. These three stations provide free daily parking for MARTA passengers, and are located in heavily dense areas with many businesses.\n",
    "\n",
    "I also discovered that only Civic Center, Lindbergh Center, Inman Park/Reynoldstown, and potentially Buckhead, Midtown and Peachtree Center have Residence venues within walking distance from the train station. Georgia State is the only MARTA station with venues in College & University category.\n",
    "\n",
    "Fast food restaurants and coffee shops dominate areas around all MARTA stations. Obviously, this is where people spend most of \n",
    "their time and money, which is a good news for the fast food companies. \n",
    "\n",
    "I used TF-IDF and LDA techniques to label each MARTA station with its discriminative categories/topics. It turned out that there are many Nightlife Spots near MARTA, combined either with Arts & Entertainment, or Food venues.\n",
    "\n",
    "I created interactive Folium maps of Atlanta with information for each MARTA stations (name, venue categories-based label,  Entries/Day, number of venues, free parking) superimposed on top.\n",
    "\n",
    "I also discovered that there is a good chiropractic at Kensington station in Atlanta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for reading!\n",
    "\n",
    "This notebook was created by Nelli Fedorova(https://www.linkedin.com/in/nelli-fedorova-7710b01a/). I hope you found this report interesting and educational. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a course on **Coursera** called *Applied Data Science Capstone*. If you accessed this notebook outside the course, you can take this course online by clicking [here](http://cocl.us/DP0701EN_Coursera_Week3_LAB2)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
